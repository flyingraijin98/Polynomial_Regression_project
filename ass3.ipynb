{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradDes:\n",
    "\n",
    "  #constructor\n",
    "  def _init_(self):\n",
    "    self.X = 0\n",
    "\n",
    "  def fit(self,X,Y,learning_rate = 0.000001,num_iter = 10000):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "    self.theta_curr = np.zeros((self.X.shape[1]))\n",
    "\n",
    "    self.errors = list()\n",
    "\n",
    "    #iterating utill the difference between two errors is <= 0.001 i.e negligible\n",
    "    for i in range(num_iter +1):\n",
    "      \n",
    "      #(derivative term) = 2*(X)t.(X.(theta_curr)-(Y)t) --usually people divide derivative term with (2)\n",
    "      self.theta_new = self.theta_curr - (learning_rate)*(np.dot(self.X.transpose(),(np.dot(self.X,self.theta_curr)-self.Y.transpose())))\n",
    "\n",
    "      #caluclating error using formula ***(error) = (X.(theta)t - (Y)t).(X.(theta)t - (Y)t)t*** -- t means transpose -some people divide this by 2\n",
    "      self.error = (np.dot(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose(),(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose()).transpose()))/2\n",
    "\n",
    "      self.theta_curr = self.theta_new\n",
    "\n",
    "      #storing the error after every 50 iterations\n",
    "      if(i % 50 == 0):\n",
    "        self.errors.append(self.error)\n",
    "\n",
    "  # returns an array of [c m1 m2 m3......mn]    \n",
    "  def coefficient_(self):      \n",
    "    return self.theta_new\n",
    "\n",
    "  #returns no of interations taken and also returns an array with error every 50 iteration\n",
    "  def errors_after_50iterations(self):     \n",
    "    return np.array(self.errors)\n",
    "\n",
    "  #returns an array of predicted values\n",
    "  def get_predicted_values(self,x):\n",
    "    return np.dot(x,self.theta_new.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class StochasticGD:\n",
    "\n",
    "  #constructor\n",
    "  def _init_(self):\n",
    "    self.X = 0\n",
    "\n",
    "  #returns nothing\n",
    "  def fit(self,X,Y,learning_rate = 0.000001,num_iter = 10000):\n",
    "    \n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "    self.theta_curr = np.zeros((self.X.shape[1]))   \n",
    "\n",
    "    self.errors = list()\n",
    "\n",
    "    for i in range(num_iter + 1):\n",
    "      self.Xp = list()\n",
    "      self.Yp = list()\n",
    "      random_no = random.randint(0,len(self.Y)-1)\n",
    "      self.Xp.append(X[random_no])\n",
    "      self.Yp.append(Y[random_no])\n",
    "\n",
    "      self.Xp = np.array(self.Xp)\n",
    "      self.Yp = np.array(self.Yp)\n",
    "\n",
    "      self.theta_new = self.theta_curr - (learning_rate)*(np.dot(self.Xp.transpose(),(np.dot(self.Xp,self.theta_curr)-self.Yp.transpose())))\n",
    "\n",
    "      self.error = (np.dot(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose(),(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose()).transpose()))/2\n",
    "\n",
    "      self.theta_curr = self.theta_new\n",
    "\n",
    "      #storing the error after every 50 iterations\n",
    "      if(i % 50 == 0):\n",
    "        self.errors.append(self.error)\n",
    "\n",
    "  #returns an array of estimated parameters [c,m1,m2.....,mn]\n",
    "  def coefficient_(self): \n",
    "    return self.theta_new\n",
    "\n",
    "  #returns an array with error every 50 iteration\n",
    "  def errors_after_50iterations(self):     \n",
    "    return np.array(self.errors)\n",
    "\n",
    "  def get_predicted_values(self,x):     \n",
    "    return np.dot(x,self.theta_new.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RidgeGD:\n",
    "\n",
    "  #constructor\n",
    "  def _init_(self):\n",
    "    self.X = 0\n",
    "\n",
    "  #returns nothing\n",
    "  def fit(self,X,Y,learning_rate = 0.000001,num_iter = 10000,lambda_ridge=0.01):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "    self.theta_curr = np.zeros((self.X.shape[1]))\n",
    "\n",
    "    # making a list to store error every 50 iterations\n",
    "    self.errors = list()\n",
    "\n",
    "    for i in range(num_iter +1):\n",
    "      \n",
    "      self.theta_new = self.theta_curr - (learning_rate)*((np.dot(self.X.transpose(),(np.dot(self.X,self.theta_curr)-self.Y.transpose()))) + 2*(lambda_ridge)*(self.theta_curr))\n",
    "\n",
    "      self.error = (np.dot(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose(),(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose()).transpose()))/2 + ((lambda_ridge)*np.dot((self.theta_new),(self.theta_new.transpose())))\n",
    "\n",
    "      self.theta_curr = self.theta_new\n",
    "\n",
    "      #storing the error after every 50 iterations\n",
    "      if(i % 50 == 0):\n",
    "        self.errors.append(self.error)\n",
    "\n",
    "  # returns an array of [c m1 m2 m3......mn]    \n",
    "  def coefficient_(self):      \n",
    "    return self.theta_new\n",
    "\n",
    "  def errors_after_50iterations(self):     \n",
    "    return np.array(self.errors)\n",
    "\n",
    "  #returns an array of predicted values\n",
    "  def get_predicted_values(self,x):\n",
    "    return np.dot(x,self.theta_new.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class RidgeSGD:\n",
    "\n",
    "  #constructor\n",
    "  def _init_(self):\n",
    "    self.X = 0\n",
    "\n",
    "  #returns nothing\n",
    "  def fit(self,X,Y,learning_rate = 0.000001,num_iter = 10000,lambda_ridge = 0.01):\n",
    "    \n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "    self.theta_curr = np.zeros((self.X.shape[1]))   \n",
    "\n",
    "    # making a list to store error every 50 iterations\n",
    "    self.errors = list()\n",
    "\n",
    "    for i in range(num_iter + 1):\n",
    "      #generating a random point\n",
    "      self.Xp = list()\n",
    "      self.Yp = list()\n",
    "      random_no = random.randint(0,len(self.Y)-1)\n",
    "      self.Xp.append(X[random_no])\n",
    "      self.Yp.append(Y[random_no])\n",
    "\n",
    "      self.Xp = np.array(self.Xp)\n",
    "      self.Yp = np.array(self.Yp)\n",
    "\n",
    "      self.theta_new = self.theta_curr - (learning_rate)*((np.dot(self.Xp.transpose(),(np.dot(self.Xp,self.theta_curr)-self.Yp.transpose()))) + 2*(lambda_ridge)*(self.theta_curr))\n",
    "\n",
    "      self.error = (np.dot(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose(),(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose()).transpose()))/2 + ((lambda_ridge)*np.dot((self.theta_new),(self.theta_new.transpose())))\n",
    "\n",
    "      self.theta_curr = self.theta_new\n",
    "\n",
    "      #storing the error after every 50 iterations\n",
    "      if(i % 50 == 0):\n",
    "        self.errors.append(self.error)\n",
    "\n",
    "  #returns an array of estimated parameters [c,m1,m2.....,mn]\n",
    "  def coefficient_(self): \n",
    "    return self.theta_new\n",
    "\n",
    "  #returns an array with error every 50 iteration\n",
    "  def errors_after_50iterations(self):     \n",
    "    return np.array(self.errors)\n",
    "\n",
    "  #returns an array of predicted values\n",
    "  def get_predicted_values(self,x):     \n",
    "    return np.dot(x,self.theta_new.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LassoGD:\n",
    "\n",
    "  #constructor\n",
    "  def _init_(self):\n",
    "    self.X = 0\n",
    "\n",
    "  #returns nothing\n",
    "  def fit(self,X,Y,learning_rate = 0.000001,num_iter = 10000,lambda_lasso=0.01):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "    self.theta_curr = np.zeros((self.X.shape[1]))\n",
    "\n",
    "    # making a list to store error every 50 iterations\n",
    "    self.errors = list()\n",
    "\n",
    "    for i in range(num_iter +1):\n",
    "      \n",
    "      self.theta_new = self.theta_curr - (learning_rate)*((np.dot(self.X.transpose(),(np.dot(self.X,self.theta_curr)-self.Y.transpose()))) + (lambda_lasso)*(np.sign(self.theta_curr)))\n",
    "\n",
    "      self.error = (np.dot(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose(),(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose()).transpose()))/2 + ((lambda_lasso)*(np.abs(self.theta_new)))\n",
    "\n",
    "      self.theta_curr = self.theta_new\n",
    "\n",
    "      #storing the error after every 50 iterations\n",
    "      if(i % 50 == 0):\n",
    "        self.errors.append(self.error)\n",
    "\n",
    "  # returns an array of [c m1 m2 m3......mn]    \n",
    "  def coefficient_(self):      \n",
    "    return self.theta_new\n",
    "\n",
    "  def errors_after_50iterations(self):     \n",
    "    return np.array(self.errors)\n",
    "\n",
    "  #returns an array of predicted values\n",
    "  def get_predicted_values(self,x):\n",
    "    return np.dot(x,self.theta_new.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class LassoSGD:\n",
    "\n",
    "  #constructor\n",
    "  def _init_(self):\n",
    "    self.X = 0\n",
    "\n",
    "  #returns nothing\n",
    "  def fit(self,X,Y,learning_rate = 0.000001,num_iter = 10000,lambda_lasso = 0.01):\n",
    "    \n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "    self.theta_curr = np.zeros((self.X.shape[1]))   \n",
    "\n",
    "    # making a list to store error every 50 iterations\n",
    "    self.errors = list()\n",
    "\n",
    "    for i in range(num_iter + 1):\n",
    "      #generating a random point\n",
    "      self.Xp = list()\n",
    "      self.Yp = list()\n",
    "      random_no = random.randint(0,len(self.Y)-1)\n",
    "      self.Xp.append(X[random_no])\n",
    "      self.Yp.append(Y[random_no])\n",
    "\n",
    "      #converting list into numpy array cause numpy is very fast\n",
    "      self.Xp = np.array(self.Xp)\n",
    "      self.Yp = np.array(self.Yp)\n",
    "\n",
    "      self.theta_new = self.theta_curr - (learning_rate)*((np.dot(self.Xp.transpose(),(np.dot(self.Xp,self.theta_curr)-self.Yp.transpose()))) + (lambda_lasso)*(np.sign(self.theta_curr)))\n",
    "\n",
    "      self.error = ((np.dot(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose(),(np.dot(self.X,self.theta_new.transpose()) - self.Y.transpose()).transpose()))/2 + ((lambda_lasso)*(np.abs(self.theta_new))))\n",
    "\n",
    "      self.theta_curr = self.theta_new\n",
    "\n",
    "      #storing the error after every 50 iterations\n",
    "      if(i % 50 == 0):\n",
    "        self.errors.append(self.error)\n",
    "\n",
    "  #returns an array of estimated parameters [c,m1,m2.....,mn]\n",
    "  def coefficient_(self): \n",
    "    return self.theta_new\n",
    "\n",
    "  #returns an array with error every 50 iteration\n",
    "  def errors_after_50iterations(self):     \n",
    "    return np.array(self.errors)\n",
    "\n",
    "  #returns an array of predicted values\n",
    "  def get_predicted_values(self,x):     \n",
    "    return np.dot(x,self.theta_new.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#pass y_actual and y_pred as numpy row array\n",
    "class MetricEval:\n",
    "\n",
    "    #constructor\n",
    "    def _init_(self):\n",
    "        self.y_pred = 0\n",
    "        self.y_actual = 0\n",
    "\n",
    "    def get_rmse(self , y_actual , y_pred):\n",
    "        self.RMSE = 0;\n",
    "        self.RMSE = math.sqrt((np.sum(((y_actual - y_pred)*(y_actual - y_pred))))/len(y_actual))\n",
    "        return self.RMSE\n",
    "\n",
    "    def get_mse(self , y_actual , y_pred):\n",
    "        self.MSE = 0;\n",
    "        self.MSE = np.sum(((y_actual - y_pred)*(y_actual - y_pred)))/len(y_actual)\n",
    "        return self.MSE\n",
    "\n",
    "    def get_ssres(self , y_actual , y_pred):\n",
    "        self.SSRES = 0\n",
    "        self.SSRES = (np.sum((y_actual - y_pred)*(y_actual - y_pred)))/2\n",
    "        return self.SSRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./insurance.csv')\n",
    "df\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping the children column\n",
    "df.drop(columns=['children'],inplace=True,axis = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial(dataset , degree):\n",
    "\n",
    "  from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "  arr = np.array(dataset)\n",
    "  polynomial_ = PolynomialFeatures(degree = degree , include_bias= False)\n",
    "  df_poly = pd.DataFrame(polynomial_.fit_transform(arr),columns=polynomial_.get_feature_names(dataset.columns))\n",
    "\n",
    "  #normalizing\n",
    "  coloumn_name = np.array(df_poly.columns)\n",
    "  for i in coloumn_name:\n",
    "    col = np.array(df_poly[i])\n",
    "    mean_col = np.mean(col)\n",
    "    std_col = np.std(col)\n",
    "    norm_col = []\n",
    "    for j in col:\n",
    "        norm_col.append((j-mean_col)/std_col)\n",
    "    df_poly[i] = norm_col\n",
    "\n",
    "  #adding bias column\n",
    "  bias = [1 for i in range(1338)]\n",
    "  df_poly.insert(loc = 0 , column = \"bias\" , value = bias )\n",
    "\n",
    "  return df_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfi_x represent all independent features of i degree\n",
    "df_1_x = generate_polynomial(df.iloc[:,[0,1]] , 1)\n",
    "df_2_x = generate_polynomial(df.iloc[:,[0,1]] , 2)\n",
    "df_3_x = generate_polynomial(df.iloc[:,[0,1]] , 3)\n",
    "df_4_x = generate_polynomial(df.iloc[:,[0,1]] , 4)\n",
    "df_5_x = generate_polynomial(df.iloc[:,[0,1]] , 5)\n",
    "df_6_x = generate_polynomial(df.iloc[:,[0,1]] , 6)\n",
    "df_7_x = generate_polynomial(df.iloc[:,[0,1]] , 7)\n",
    "df_8_x = generate_polynomial(df.iloc[:,[0,1]] , 8)\n",
    "df_9_x = generate_polynomial(df.iloc[:,[0,1]] , 9)\n",
    "df_10_x = generate_polynomial(df.iloc[:,[0,1]] , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = np.array(df.iloc[:,[2]])\n",
    "mean_col = np.mean(col)\n",
    "std_col = np.std(col)\n",
    "norm_col_charges = []\n",
    "for j in col:\n",
    "  norm_col_charges.append((j-mean_col)/std_col)\n",
    "\n",
    "df_1_x['charges'] = np.array(norm_col_charges)\n",
    "df1 = df_1_x\n",
    "df1\n",
    "df_2_x['charges'] = np.array(norm_col_charges)\n",
    "df2 = df_2_x\n",
    "df2\n",
    "df_3_x['charges'] = np.array(norm_col_charges)\n",
    "df3 = df_3_x\n",
    "df3\n",
    "df_4_x['charges'] = np.array(norm_col_charges)\n",
    "df4 = df_4_x\n",
    "df4\n",
    "df_5_x['charges'] = np.array(norm_col_charges)\n",
    "df5 = df_5_x\n",
    "df5\n",
    "df_6_x['charges'] = np.array(norm_col_charges)\n",
    "df6 = df_6_x\n",
    "df6\n",
    "df_7_x['charges'] = np.array(norm_col_charges)\n",
    "df7 = df_7_x\n",
    "df7\n",
    "df_8_x['charges'] = np.array(norm_col_charges)\n",
    "df8 = df_8_x\n",
    "df8\n",
    "df_9_x['charges'] = np.array(norm_col_charges)\n",
    "df9 = df_9_x\n",
    "df9\n",
    "df_10_x['charges'] = np.array(norm_col_charges)\n",
    "df10 = df_10_x\n",
    "df10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating objects of the user defined class\n",
    "regressorGD = GradDes()\n",
    "m = MetricEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_poly(df):\n",
    "\n",
    "  #test train split\n",
    "  df_train = df.iloc[0:936]\n",
    "  df_test = df.iloc[936:]\n",
    "  x_train = np.array(df_train.iloc[:,:-1])\n",
    "  y_train = np.array(df_train.iloc[:,-1])\n",
    "  x_test = np.array(df_test.iloc[:,:-1])\n",
    "  y_test = np.array(df_test.iloc[:,-1])\n",
    "  \n",
    "  #training\n",
    "  regressorGD.fit(x_train,y_train,0.000001,20000)\n",
    "  coef = regressorGD.coefficient_()\n",
    "  errors_after_50iterations = regressorGD.errors_after_50iterations()\n",
    "  print('Error Every 50 Epochs : {}'.format(errors_after_50iterations))\n",
    "  print(\"\")\n",
    "  print('Cofficients : {}'.format(coef))\n",
    "  epochs = [50*i for i in range(len(errors_after_50iterations))]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Error\")\n",
    "  plt.title(\"Error V/S Epoch\",fontsize = 20)\n",
    "  plt.plot(epochs,errors_after_50iterations,color='green',linestyle=\"--\",marker='o')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  y_predicted_training = regressorGD.get_predicted_values(x_train)\n",
    "  print('RMSE--Train: {}'.format(m.get_rmse(y_train,y_predicted_training)))\n",
    "  print('MSE--Train: {}'.format(m.get_mse(y_train,y_predicted_training)))\n",
    "  print('Total Error--Train: {}'.format(m.get_ssres(y_train,y_predicted_training)))\n",
    "  print(\"\")\n",
    "\n",
    "  #calculating accuracy for testing dataset\n",
    "  y_predicted_testing = regressorGD.get_predicted_values(x_test)\n",
    "  #print(y_test)\n",
    "  print('RMSE--Test: {}'.format(m.get_rmse(y_test,y_predicted_testing)))\n",
    "  print('MSE--Test: {}'.format(m.get_mse(y_test,y_predicted_testing)))\n",
    "  print('Total Error--Test: {}'.format(m.get_ssres(y_test,y_predicted_testing)))\n",
    "  print(\"\")\n",
    "\n",
    "  #surface plots of training dataset\n",
    "  fig = plt.figure(figsize=(9,6))\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "  x = np.array((pd.DataFrame(x_train)).iloc[:,1])\n",
    "  y = np.array((pd.DataFrame(x_train)).iloc[:,2])\n",
    "  ax.set_xlabel('Age', fontsize=15, color = \"black\",y=5)\n",
    "  ax.set_ylabel('BMI', fontsize = 15, color = \"black\",y=5)\n",
    "  ax.set_zlabel('Charges', fontsize=15, color = \"black\",y=5)\n",
    "  plt.scatter(x,y,y_train,zdir='z',color = 'red')\n",
    "  trisurf = ax.plot_trisurf(x,y,y_predicted_training,cmap = plt.get_cmap('ocean'))\n",
    "  plt.show()\n",
    "\n",
    "print(\" *** ***  1 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df1)\n",
    "\n",
    "print(\" *** ***  2 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df2)\n",
    "\n",
    "print(\" *** ***  3 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df3)\n",
    "\n",
    "print(\" *** ***  4 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df4)\n",
    "\n",
    "print(\" *** ***  5 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df5)\n",
    "\n",
    "print(\" *** ***  6 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df6)\n",
    "\n",
    "print(\" *** ***  7 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df7)\n",
    "\n",
    "print(\" *** ***  8 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df8)\n",
    "\n",
    "print(\" *** ***  9 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df9)\n",
    "\n",
    "print(\" *** ***  10 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochasticgradientdescent_regressor = StochasticGD()\n",
    "m = MetricEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_poly(df):\n",
    "\n",
    "  #test train split\n",
    "  df_train = df.iloc[0:936]\n",
    "  df_test = df.iloc[936:]\n",
    "  x_train = np.array(df_train.iloc[:,:-1])\n",
    "  y_train = np.array(df_train.iloc[:,-1])\n",
    "  x_test = np.array(df_test.iloc[:,:-1])\n",
    "  y_test = np.array(df_test.iloc[:,-1])\n",
    "\n",
    "  #training\n",
    "  stochasticgradientdescent_regressor.fit(x_train,y_train,0.0001,20000)\n",
    "  coef = stochasticgradientdescent_regressor.coefficient_()\n",
    "  errors_after_50iterations = stochasticgradientdescent_regressor.errors_after_50iterations()\n",
    "  #print(\"\")\n",
    "  print('Cofficients : {}'.format(coef))\n",
    "  epochs = [50*i for i in range(len(errors_after_50iterations))]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Error\")\n",
    "  plt.title(\"Error V/S Epoch\",fontsize = 20)\n",
    "  plt.plot(epochs,errors_after_50iterations,color='green',linestyle=\"--\",marker='o')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  #calculating accuracy for training dataset\n",
    "  y_predicted_training = stochasticgradientdescent_regressor.get_predicted_values(x_train)\n",
    "  print('RMSE--Train: {}'.format(m.get_rmse(y_train,y_predicted_training)))\n",
    "  print('MSE--Train: {}'.format(m.get_mse(y_train,y_predicted_training)))\n",
    "  print('Total Error--Train: {}'.format(m.get_ssres(y_train,y_predicted_training)))\n",
    "  print(\"\")\n",
    "\n",
    "  #calculating accuracy for testing dataset\n",
    "  y_predicted_testing = stochasticgradientdescent_regressor.get_predicted_values(x_test)\n",
    "  #print(y_test)\n",
    "  #print(y_predicted_testing)\n",
    "  print('RMSE--Test: {}'.format(m.get_rmse(y_test,y_predicted_testing)))\n",
    "  print('MSE--Test: {}'.format(m.get_mse(y_test,y_predicted_testing)))\n",
    "  print('Total Error--Test: {}'.format(m.get_ssres(y_test,y_predicted_testing)))\n",
    "  print(\"\")\n",
    "\n",
    "print(\" *** ***  1 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df1)\n",
    "\n",
    "print(\" *** ***  2 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df2)\n",
    "\n",
    "print(\" *** ***  3 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df3)\n",
    "\n",
    "print(\" *** ***  4 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df4)\n",
    "\n",
    "print(\" *** ***  5 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df5)\n",
    "\n",
    "print(\" *** ***  6 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df6)\n",
    "\n",
    "print(\" *** ***  7 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df7)\n",
    "\n",
    "print(\" *** ***  8 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df8)\n",
    "\n",
    "print(\" *** ***  9 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df9)\n",
    "\n",
    "print(\" *** ***  10 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating objects of the user defined class\n",
    "gradientdescent_ridge_regressor = RidgeGD()\n",
    "m = MetricEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training different degree polynomials\n",
    "def train_poly(df):\n",
    "  \n",
    "  df_train = df.iloc[0:936]\n",
    "  df_validation = df.iloc[936:1203]\n",
    "  df_test = df.iloc[1203:]\n",
    "  x_train = np.array(df_train.iloc[:,:-1])\n",
    "  y_train = np.array(df_train.iloc[:,-1])\n",
    "  x_validation = np.array(df_validation.iloc[:,:-1])\n",
    "  y_validation = np.array(df_validation.iloc[:,-1])\n",
    "  x_test = np.array(df_test.iloc[:,:-1])\n",
    "  y_test = np.array(df_test.iloc[:,-1])\n",
    "\n",
    "  #hyperparameter tuning\n",
    "  hyperparameter = np.random.uniform(0,1,10)\n",
    "  accuracies_hyperparameter = list()\n",
    "  for i in hyperparameter:\n",
    "    gradientdescent_ridge_regressor.fit(x_train,y_train,0.000001,20000,i)\n",
    "    y_pred_validation = gradientdescent_ridge_regressor.get_predicted_values(x_validation)\n",
    "    accuracies_hyperparameter.append([m.get_rmse(y_validation,y_pred_validation),i])\n",
    "  accuracies_hyperparameter = sorted(accuracies_hyperparameter,key=lambda x : x[0],reverse=True) \n",
    "  print(\"[rmse_validation_error , hyperparameter_value] : {}\".format(accuracies_hyperparameter))\n",
    "  best_hyperparameter = accuracies_hyperparameter[9][1]\n",
    "  print(\"\")\n",
    "\n",
    "  #training with best regularization parameter\n",
    "  print(\"Training with best hyperparameter found : {}\".format(best_hyperparameter))\n",
    "  gradientdescent_ridge_regressor.fit(x_train,y_train,0.000001,20000,best_hyperparameter)\n",
    "  coef = gradientdescent_ridge_regressor.coefficient_()\n",
    "  errors_after_50iterations = gradientdescent_ridge_regressor.errors_after_50iterations()\n",
    "  #print(\"\")\n",
    "  print('Cofficients : {}'.format(coef))\n",
    "  epochs = [50*i for i in range(len(errors_after_50iterations))]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Error\")\n",
    "  plt.title(\"Error V/S Epoch\",fontsize = 20)\n",
    "  plt.plot(epochs,errors_after_50iterations,color='green',linestyle=\"--\",marker='o')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  #calculating accuracy for training dataset\n",
    "  y_predicted_training = gradientdescent_ridge_regressor.get_predicted_values(x_train)\n",
    "  #print(y_train)\n",
    "  #print(y_predicted_training)\n",
    "  print('RMSE--Train: {}'.format(m.get_rmse(y_train,y_predicted_training)))\n",
    "  print('MSE--Train: {}'.format(m.get_mse(y_train,y_predicted_training)))\n",
    "  print('Total Error--Train: {}'.format(m.get_ssres(y_train,y_predicted_training)))\n",
    "  print(\"\")\n",
    "\n",
    "  y_predicted_testing = gradientdescent_ridge_regressor.get_predicted_values(x_test)\n",
    "  #print(y_test)\n",
    "  #print(y_predicted_testing)\n",
    "  print('RMSE--Test: {}'.format(m.get_rmse(y_test,y_predicted_testing)))\n",
    "  print('MSE--Test: {}'.format(m.get_mse(y_test,y_predicted_testing)))\n",
    "  print('Total Error--Test: {}'.format(m.get_ssres(y_test,y_predicted_testing)))\n",
    "  print(\"\")\n",
    "\n",
    "print(\" *** ***  1 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df1)\n",
    "\n",
    "print(\" *** ***  2 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df2)\n",
    "\n",
    "print(\" *** ***  3 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df3)\n",
    "\n",
    "print(\" *** ***  4 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df4)\n",
    "\n",
    "print(\" *** ***  5 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df5)\n",
    "\n",
    "print(\" *** ***  6 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df6)\n",
    "\n",
    "print(\" *** ***  7 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df7)\n",
    "\n",
    "print(\" *** ***  8 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df8)\n",
    "\n",
    "print(\" *** ***  9 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df9)\n",
    "\n",
    "print(\" *** ***  10 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating objects of the user defined class\n",
    "stochasticgradientdescent_ridge_regressor = RidgeSGD()\n",
    "m = MetricEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training different degree polynomials\n",
    "def train_poly(df):\n",
    "  \n",
    "  #train validation test split\n",
    "  df_train = df.iloc[0:936]\n",
    "  df_validation = df.iloc[936:1203]\n",
    "  df_test = df.iloc[1203:]\n",
    "  x_train = np.array(df_train.iloc[:,:-1])\n",
    "  y_train = np.array(df_train.iloc[:,-1])\n",
    "  x_validation = np.array(df_validation.iloc[:,:-1])\n",
    "  y_validation = np.array(df_validation.iloc[:,-1])\n",
    "  x_test = np.array(df_test.iloc[:,:-1])\n",
    "  y_test = np.array(df_test.iloc[:,-1])\n",
    "\n",
    "  #hyperparameter tuning\n",
    "  hyperparameter = np.random.uniform(0,1,10)\n",
    "  accuracies_hyperparameter = list()\n",
    "  for i in hyperparameter:\n",
    "    stochasticgradientdescent_ridge_regressor.fit(x_train,y_train,0.000001,20000,i)\n",
    "    y_pred_validation = stochasticgradientdescent_ridge_regressor.get_predicted_values(x_validation)\n",
    "    accuracies_hyperparameter.append([m.get_rmse(y_validation,y_pred_validation),i])\n",
    "  accuracies_hyperparameter = sorted(accuracies_hyperparameter,key=lambda x : x[0],reverse=True) \n",
    "  print(\"[rmse_validation_error , hyperparameter_value] : {}\".format(accuracies_hyperparameter))\n",
    "  best_hyperparameter = accuracies_hyperparameter[9][1]\n",
    "  print(\"\")\n",
    "\n",
    "  #training with best regularization parameter\n",
    "  print(\"Training with best hyperparameter found : {}\".format(best_hyperparameter))\n",
    "  stochasticgradientdescent_ridge_regressor.fit(x_train,y_train,0.0001,20000,best_hyperparameter)\n",
    "  coef = stochasticgradientdescent_ridge_regressor.coefficient_()\n",
    "  errors_after_50iterations = stochasticgradientdescent_ridge_regressor.errors_after_50iterations()\n",
    "  #print('Error Every 50 Epochs : {}'.format(errors_after_50iterations))\n",
    "  #print(\"\")\n",
    "  print('Cofficients : {}'.format(coef))\n",
    "  epochs = [50*i for i in range(len(errors_after_50iterations))]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Error\")\n",
    "  plt.title(\"Error V/S Epoch\",fontsize = 20)\n",
    "  plt.plot(epochs,errors_after_50iterations,color='green',linestyle=\"--\",marker='o')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  #calculating accuracy for training dataset\n",
    "  y_predicted_training = stochasticgradientdescent_ridge_regressor.get_predicted_values(x_train)\n",
    "  #print(y_train)\n",
    "  #print(y_predicted_training)\n",
    "  print('RMSE--Train: {}'.format(m.get_rmse(y_train,y_predicted_training)))\n",
    "  print('MSE--Train: {}'.format(m.get_mse(y_train,y_predicted_training)))\n",
    "  print('Total Error--Train: {}'.format(m.get_ssres(y_train,y_predicted_training)))\n",
    "  print(\"\")\n",
    "\n",
    "  #calculating accuracy for testing dataset\n",
    "  y_predicted_testing = stochasticgradientdescent_ridge_regressor.get_predicted_values(x_test)\n",
    "  #print(y_test)\n",
    "  #print(y_predicted_testing)\n",
    "  print('RMSE--Test: {}'.format(m.get_rmse(y_test,y_predicted_testing)))\n",
    "  print('MSE--Test: {}'.format(m.get_mse(y_test,y_predicted_testing)))\n",
    "  print('Total Error--Test: {}'.format(m.get_ssres(y_test,y_predicted_testing)))\n",
    "  print(\"\")\n",
    "\n",
    "print(\" *** ***  1 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df1)\n",
    "\n",
    "print(\" *** ***  2 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df2)\n",
    "\n",
    "print(\" *** ***  3 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df3)\n",
    "\n",
    "print(\" *** ***  4 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df4)\n",
    "\n",
    "print(\" *** ***  5 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df5)\n",
    "\n",
    "print(\" *** ***  6 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df6)\n",
    "\n",
    "print(\" *** ***  7 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df7)\n",
    "\n",
    "print(\" *** ***  8 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df8)\n",
    "\n",
    "print(\" *** ***  9 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df9)\n",
    "\n",
    "print(\" *** ***  10 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating objects of the user defined class\n",
    "gradientdescent_lasso_regressor = LassoGD()\n",
    "m = MetricEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training different degree polynomials\n",
    "def train_poly(df):\n",
    "  \n",
    "  #train validation test split\n",
    "  df_train = df.iloc[0:936]\n",
    "  df_validation = df.iloc[936:1203]\n",
    "  df_test = df.iloc[1203:]\n",
    "  x_train = np.array(df_train.iloc[:,:-1])\n",
    "  y_train = np.array(df_train.iloc[:,-1])\n",
    "  x_validation = np.array(df_validation.iloc[:,:-1])\n",
    "  y_validation = np.array(df_validation.iloc[:,-1])\n",
    "  x_test = np.array(df_test.iloc[:,:-1])\n",
    "  y_test = np.array(df_test.iloc[:,-1])\n",
    "\n",
    "  #hyperparameter tuning\n",
    "  hyperparameter = np.random.uniform(0,1,10)\n",
    "  accuracies_hyperparameter = list()\n",
    "  for i in hyperparameter:\n",
    "    gradientdescent_lasso_regressor.fit(x_train,y_train,0.000001,20000,i)\n",
    "    y_pred_validation = gradientdescent_lasso_regressor.get_predicted_values(x_validation)\n",
    "    accuracies_hyperparameter.append([m.get_rmse(y_validation,y_pred_validation),i])\n",
    "  accuracies_hyperparameter = sorted(accuracies_hyperparameter,key=lambda x : x[0],reverse=True) \n",
    "  print(\"[rmse_validation_error , hyperparameter_value] : {}\".format(accuracies_hyperparameter))\n",
    "  best_hyperparameter = accuracies_hyperparameter[9][1]\n",
    "  print(\"\")\n",
    "\n",
    "  #training with best regularization parameter\n",
    "  print(\"Training with best hyperparameter found : {}\".format(best_hyperparameter))\n",
    "  gradientdescent_lasso_regressor.fit(x_train,y_train,0.000001,20000,best_hyperparameter)\n",
    "  coef = gradientdescent_lasso_regressor.coefficient_()\n",
    "  errors_after_50iterations = gradientdescent_lasso_regressor.errors_after_50iterations()\n",
    "  #print('Error Every 50 Epochs : {}'.format(errors_after_50iterations))\n",
    "  #print(\"\")\n",
    "  print('Cofficients : {}'.format(coef))\n",
    "  epochs = [50*i for i in range(len(errors_after_50iterations))]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Error\")\n",
    "  plt.title(\"Error V/S Epoch\",fontsize = 20)\n",
    "  plt.plot(epochs,errors_after_50iterations,color='green',linestyle=\"--\",marker='o')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  y_predicted_training = gradientdescent_lasso_regressor.get_predicted_values(x_train)\n",
    "  #print(y3_train)\n",
    "  #print(y3_pred_train)\n",
    "  print('RMSE--Train: {}'.format(m.get_rmse(y_train,y_predicted_training)))\n",
    "  print('MSE--Train: {}'.format(m.get_mse(y_train,y_predicted_training)))\n",
    "  print('Total Error--Train: {}'.format(m.get_ssres(y_train,y_predicted_training)))\n",
    "  print(\"\")\n",
    "\n",
    "  y_predicted_testing = gradientdescent_lasso_regressor.get_predicted_values(x_test)\n",
    "  #print(y3_test)\n",
    "  #print(y3_pred_test)\n",
    "  print('RMSE--Test: {}'.format(m.get_rmse(y_test,y_predicted_testing)))\n",
    "  print('MSE--Test: {}'.format(m.get_mse(y_test,y_predicted_testing)))\n",
    "  print('Total Error--Test: {}'.format(m.get_ssres(y_test,y_predicted_testing)))\n",
    "  print(\"\")\n",
    "\n",
    "print(\" *** ***  1 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df1)\n",
    "\n",
    "print(\" *** ***  2 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df2)\n",
    "\n",
    "print(\" *** ***  3 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df3)\n",
    "\n",
    "print(\" *** ***  4 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df4)\n",
    "\n",
    "print(\" *** ***  5 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df5)\n",
    "\n",
    "print(\" *** ***  6 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df6)\n",
    "\n",
    "print(\" *** ***  7 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df7)\n",
    "\n",
    "print(\" *** ***  8 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df8)\n",
    "\n",
    "print(\" *** ***  9 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df9)\n",
    "\n",
    "print(\" *** ***  10 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating objects of the user defined class\n",
    "stochasticgradientdescent_lasso_regressor = LassoSGD()\n",
    "m = MetricEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training different degree polynomials\n",
    "def train_poly(df):\n",
    "  \n",
    "  #train validation test split\n",
    "  df_train = df.iloc[0:936]\n",
    "  df_validation = df.iloc[936:1203]\n",
    "  df_test = df.iloc[1203:]\n",
    "  x_train = np.array(df_train.iloc[:,:-1])\n",
    "  y_train = np.array(df_train.iloc[:,-1])\n",
    "  x_validation = np.array(df_validation.iloc[:,:-1])\n",
    "  y_validation = np.array(df_validation.iloc[:,-1])\n",
    "  x_test = np.array(df_test.iloc[:,:-1])\n",
    "  y_test = np.array(df_test.iloc[:,-1])\n",
    "\n",
    "  #hyperparameter tuning\n",
    "  hyperparameter = np.random.uniform(0,1,10)\n",
    "  accuracies_hyperparameter = list()\n",
    "  for i in hyperparameter:\n",
    "    stochasticgradientdescent_lasso_regressor.fit(x_train,y_train,0.000001,20000,i)\n",
    "    y_pred_validation = stochasticgradientdescent_lasso_regressor.get_predicted_values(x_validation)\n",
    "    accuracies_hyperparameter.append([m.get_rmse(y_validation,y_pred_validation),i])\n",
    "  accuracies_hyperparameter = sorted(accuracies_hyperparameter,key=lambda x : x[0],reverse=True) \n",
    "  print(\"[rmse_validation_error , hyperparameter_value] : {}\".format(accuracies_hyperparameter))\n",
    "  best_hyperparameter = accuracies_hyperparameter[9][1]\n",
    "  print(\"\")\n",
    "\n",
    "  #training using best regularization parameter\n",
    "  print(\"Training with best hyperparameter found : {}\".format(best_hyperparameter))\n",
    "  stochasticgradientdescent_lasso_regressor.fit(x_train,y_train,0.0001,20000,best_hyperparameter)\n",
    "  coef = stochasticgradientdescent_lasso_regressor.coefficient_()\n",
    "  errors_after_50iterations = stochasticgradientdescent_lasso_regressor.errors_after_50iterations()\n",
    "  #print('Error Every 50 Epochs : {}'.format(errors_after_50iterations))\n",
    "  #print(\"\")\n",
    "  print('Cofficients : {}'.format(coef))\n",
    "  epochs = [50*i for i in range(len(errors_after_50iterations))]\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Error\")\n",
    "  plt.title(\"Error V/S Epoch\",fontsize = 20)\n",
    "  plt.plot(epochs,errors_after_50iterations,color='green',linestyle=\"--\",marker='o')\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "  y_predicted_training = stochasticgradientdescent_lasso_regressor.get_predicted_values(x_train)\n",
    "  #print(y_train)\n",
    "  #print(y_predicted_training)\n",
    "  print('RMSE--Train: {}'.format(m.get_rmse(y_train,y_predicted_training)))\n",
    "  print('MSE--Train: {}'.format(m.get_mse(y_train,y_predicted_training)))\n",
    "  print('Total Error--Train: {}'.format(m.get_ssres(y_train,y_predicted_training)))\n",
    "  print(\"\")\n",
    "\n",
    "  #calculating accuracy for testing dataset\n",
    "  y_predicted_testing = stochasticgradientdescent_lasso_regressor.get_predicted_values(x_test)\n",
    "  #print(y_test)\n",
    "  #print(y_predicted_testing)\n",
    "  print('RMSE--Test: {}'.format(m.get_rmse(y_test,y_predicted_testing)))\n",
    "  print('MSE--Test: {}'.format(m.get_mse(y_test,y_predicted_testing)))\n",
    "  print('Total Error--Test: {}'.format(m.get_ssres(y_test,y_predicted_testing)))\n",
    "  print(\"\")\n",
    "\n",
    "print(\" *** ***  1 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df1)\n",
    "\n",
    "print(\" *** ***  2 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df2)\n",
    "\n",
    "print(\" *** ***  3 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df3)\n",
    "\n",
    "print(\" *** ***  4 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df4)\n",
    "\n",
    "print(\" *** ***  5 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df5)\n",
    "\n",
    "print(\" *** ***  6 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df6)\n",
    "\n",
    "print(\" *** ***  7 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df7)\n",
    "\n",
    "print(\" *** ***  8 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df8)\n",
    "\n",
    "print(\" *** ***  9 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df9)\n",
    "\n",
    "print(\" *** ***  10 DEGREE POLYNOMIAL *** *** \")\n",
    "train_poly(df10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.7 64-bit",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}